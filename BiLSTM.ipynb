{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dictionary"
      ],
      "metadata": {
        "id": "Ntt5ZVubJWMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1NZOaYcJTGe"
      },
      "outputs": [],
      "source": [
        "replacement_dict_labels = {\n",
        "  \"mở ngoặc đơn\": \"1 1 1\",\n",
        "  \"đóng ngoặc đơn\": \"2 2 2\",\n",
        "  \"mở ngoặc nhọn\": \"3 3 3\",\n",
        "  \"đóng ngoặc nhọn\": \"4 4 4\",\n",
        "  \"mở ngoặc vuông\": \"5 5 5\",\n",
        "  \"đóng ngoặc vuông\": \"6 6 6\",\n",
        "  \"gạch ngang trên\": \"7 7 7\",\n",
        "  \"gạch ngang dưới\": \"8 8 8\",\n",
        "  \"hai chấm\": \"9 9\",\n",
        "  \"chấm phẩy\": \"10 10\",\n",
        "  \"phẩy\": \"11\",\n",
        "  \"lớn hơn\": \"12 12\",\n",
        "  \"bé hơn\": \"13 13\",\n",
        "  \"chấm hỏi\": \"14 14\",\n",
        "  \"chấm than\": \"15 15\",\n",
        "  \"a còng\": \"16 16\",\n",
        "  \"dấu thăng\": \"17 17\",\n",
        "  \"phần trăm\": \"18 18\",\n",
        "  \"ba chấm\": \"19 19\",\n",
        "  \"chấm\": \"20\",\n",
        "  \"bằng\": \"21\",\n",
        "  \"xuyệt trái\": \"22 22\",\n",
        "  \"xuyệt phải\": \"23 23\"\n",
        "}\n",
        "\n",
        "replacement_dict = {\n",
        "  r\"(\": \"mở ngoặc đơn\",\n",
        "  r\")\": \"đóng ngoặc đơn\",\n",
        "  r\"{\": \"mở ngoặc nhọn\",\n",
        "  r\"}\": \"đóng ngoặc nhọn\",\n",
        "  r\"[\": \"mở ngoặc vuông\",\n",
        "  r\"]\": \"đóng ngoặc vuông\",\n",
        "  r\"-\": \"gạch ngang trên\",\n",
        "  r\"_\": \"gạch ngang dưới\",\n",
        "  r\":\": \"hai chấm\",\n",
        "  r\";\": \"chấm phẩy\",\n",
        "  r\",\": \"phẩy\",\n",
        "  r\">\": \"lớn hơn\",\n",
        "  r\"<\": \"bé hơn\",\n",
        "  r\"?\": \"chấm hỏi\",\n",
        "  r\"!\": \"chấm than\",\n",
        "  r\"@\": \"a còng\",\n",
        "  r\"#\": \"dấu thăng\",\n",
        "  r\"%\": \"phần trăm\",\n",
        "  r\"...\": \"ba chấm\",\n",
        "  r\".\": \"chấm\",\n",
        "  r\"=\": \"bằng\",\n",
        "  r\"/\": \"xuyệt trái\",\n",
        "  r\"\\\\\": \"xuyệt phải\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data process"
      ],
      "metadata": {
        "id": "FvYaTcRIJhLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert from text to number"
      ],
      "metadata": {
        "id": "jP_qgxzWJpG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def replace_special_characters_regex(text):\n",
        "  regex = re.compile(r'[\\(\\)\\[\\]{}:;,><=_?!@#%/\\\\.-]+|\\.{3}')\n",
        "  txt = regex.sub(lambda x: f' {replacement_dict.get(x.group(), x.group())} ', text)\n",
        "  txt1 = delete_space(txt)\n",
        "  return txt1\n",
        "\n",
        "def delete_space(text):\n",
        "    text_del_sp = text.strip()\n",
        "    text_del_sp_inside = re.sub(' +', ' ', text_del_sp)\n",
        "    return text_del_sp_inside\n",
        "\n",
        "def word_to_number(text):\n",
        "  pattern = \"(\" + \"|\".join(key.strip() for key in replacement_dict_labels.keys()) + \")\"\n",
        "  matches = re.findall(pattern, text)\n",
        "  result = re.sub(pattern, lambda x: replacement_dict_labels[x.group()], text)\n",
        "  return result\n",
        "\n",
        "def lower_string(text):\n",
        "    return text.lower()\n",
        "\n",
        "def text_to_0(sen):\n",
        "  ls = sen.split()\n",
        "  for i in range(len(ls)):\n",
        "    if ls[i].isnumeric() is False:\n",
        "      ls[i] = '0 '\n",
        "      i+=1\n",
        "    sen = ' '.join(ls)\n",
        "    new_sen = re.sub(' +', ' ', sen)\n",
        "  return new_sen\n",
        "\n",
        "def num_to_0(text):\n",
        "  ls = text.split()\n",
        "  for i in range(len(ls)):\n",
        "    if ls[i].isnumeric() == True:\n",
        "      ls[i] = '0 '\n",
        "      i+=1\n",
        "    sen = ' '.join(ls)\n",
        "    new_sen = re.sub(' +', ' ', sen)\n",
        "  return new_sen\n",
        "\n",
        "def convert_text_to_num(text):\n",
        "  txt1 = replace_special_characters_regex(text)\n",
        "  txt2 = lower_string(txt1)\n",
        "  txt3 = num_to_0(txt2)\n",
        "  txt4 = word_to_number(txt3)\n",
        "  txt5 = text_to_0(txt4)\n",
        "  return  text + txt1 + '\\n'\n",
        "\n",
        "'''text='Chây ì nộp phạt nguội 12/2.'\n",
        "print(convert_text_to_num(text))'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8GOkQ0G0Jm6l",
        "outputId": "bb551f05-5237-470b-c807-4a7e6887e969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"text='Chây ì nộp phạt nguội 12/2.'\\nprint(convert_text_to_num(text))\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## convert number to text"
      ],
      "metadata": {
        "id": "9Ubaonm2JumR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_lables_to_word(text):\n",
        "  for chu, so in replacement_dict_labels.items():\n",
        "    text = text.replace(so, chu)\n",
        "    text = text.replace(\" 20\", \" chấm\")\n",
        "    txt = delete_space(text)\n",
        "  return txt\n",
        "\n",
        "def form_0_to_text(num, text):\n",
        "  ls1 = num.split()\n",
        "  ls2 = text.split()\n",
        "  i = 0\n",
        "  j = 0\n",
        "  for i in range(len(ls1)):\n",
        "    if ls1[i] == '0':\n",
        "      ls1[i] = ls2[j]\n",
        "      i+=1\n",
        "      j+=1\n",
        "    else:\n",
        "      i+=1\n",
        "      j+=1\n",
        "    sentence = ' '.join(ls1)\n",
        "  return sentence\n",
        "\n",
        "def back_to_original_regex(text):\n",
        "  pattern = \"(\" + \"|\".join(re.escape(value) for value in replacement_dict.values()) + \")\"\n",
        "  result = re.sub(pattern, lambda x: next(key for key, value in replacement_dict.items() if value == x.group()), text)\n",
        "  s0 = re.sub(r'\\s*([\\(\\[\\{<])\\s*', r' \\1', result)\n",
        "  s1 = re.sub(r'\\s*([\\)\\]\\}>])\\s*', r'\\1 ', s0)\n",
        "  s2 = re.sub(r'\\s*([:;,_?!%.])\\s*', r'\\1 ', s1)\n",
        "  s3 = re.sub(r'\\s*([@#/\\\\])\\s*', r'\\1', s2)\n",
        "  s4 = re.sub(r'\\s*([-<>=])\\s*', r' \\1 ', s3)\n",
        "  s5 = re.sub(' +', ' ',s4)\n",
        "  return s5\n",
        "\n",
        "def num_to_text(text, num):\n",
        "  t = replace_lables_to_word(num)\n",
        "  t1 = form_0_to_text(t, text)\n",
        "  t2 = back_to_original_regex(t1)\n",
        "  return t2"
      ],
      "metadata": {
        "id": "gyiYmbGKJxKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare data"
      ],
      "metadata": {
        "id": "BxZ6nptBKPzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    processed_lines = [convert_text_to_num(line) for line in lines]\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(processed_lines)\n",
        "\n",
        "process_file('demo-title.txt', 'title_num.txt')"
      ],
      "metadata": {
        "id": "uvCNkOsGKRsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model BiLSTM"
      ],
      "metadata": {
        "id": "QQBw0f5jSZAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# Load the content of the file\n",
        "with open('title_num.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "# Preprocess the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(content)\n",
        "sequences = tokenizer.texts_to_sequences(content)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Split into X and y\n",
        "X = []\n",
        "y = []\n",
        "for seq in sequences:\n",
        "    if len(seq) > 1:\n",
        "        X.append(seq[:-1])\n",
        "        y.append(seq[1:])\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_len = max([len(seq) for seq in X])\n",
        "X = pad_sequences(X, maxlen=max_len, padding='post')\n",
        "y = pad_sequences(y, maxlen=max_len, padding='post')\n",
        "\n",
        "# Convert y to one-hot encoding\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Build the BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=30, verbose=1)\n",
        "\n",
        "# Generate new text\n",
        "seed_text = \"Chây ì nộp phạt nguội.\"\n",
        "next_words = 20"
      ],
      "metadata": {
        "id": "zhawcHwWE8BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b88518-8c52-47e5-f317-8f23eedb40fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "63/63 [==============================] - 22s 265ms/step - loss: 4.0085 - accuracy: 0.5867\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - 17s 267ms/step - loss: 2.8163 - accuracy: 0.5984\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - 16s 255ms/step - loss: 2.7463 - accuracy: 0.6000\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - 20s 320ms/step - loss: 2.7076 - accuracy: 0.6055\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - 16s 259ms/step - loss: 2.6813 - accuracy: 0.6107\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - 16s 259ms/step - loss: 2.6539 - accuracy: 0.6128\n",
            "Epoch 7/30\n",
            "31/63 [=============>................] - ETA: 7s - loss: 2.6426 - accuracy: 0.6107"
          ]
        }
      ]
    }
  ]
}